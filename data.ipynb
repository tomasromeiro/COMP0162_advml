{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages, establish connection to WRDS and set overall configurations for the notebook\n",
    "\n",
    "WRDS Support - https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-python/querying-wrds-data-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wrds\n",
    "import yfinance as yf \n",
    "\n",
    "# Build WRDS connection\n",
    "\n",
    "db = wrds.Connection(wrds_username='tomasromeiro')\n",
    "#db.close()\n",
    "\n",
    "# Set option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Set option to force dataframes to display numbers as floats with thousands separators\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)  # Adjust decimal places as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRDS Quick commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List libraries available\n",
    "sorted(db.list_libraries()) \n",
    "\n",
    "# List tables within a library\n",
    "db.list_tables(library=\"cboe\") \n",
    "\n",
    "# describe table metadata\n",
    "db.describe_table(library=\"cboe\", table=\"optprice_2024\") \n",
    "\n",
    "# Execute a sql query against a table (join queries between tables in library can also be performed)\n",
    "data = db.raw_sql('SELECT date, dji FROM djones.djdaily LIMIT 1', date_cols=['date']) \n",
    "\n",
    "# Pass parameters to a sql statement\n",
    "params = {\"tickers\": (\"0015B\", \"0030B\", \"0032A\", \"0033A\", \"0038A\")}\n",
    "data = db.raw_sql(\n",
    "    \"SELECT datadate, gvkey, cusip FROM comp.funda WHERE tic IN %(tickers)s LIMIT 1\",\n",
    "    params=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FINRA Short Interest Bimonthly Data  \n",
    "- https://www.finra.org/finra-data/browse-catalog/equity-short-interest/files\n",
    "- https://www.finra.org/finra-data/browse-catalog/equity-short-interest/glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Collate semi monthly datasets\n",
    "If the data has already been collated do not run this and skip to b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = 'data/finra_short_interest_data'\n",
    "output_file = os.path.join(directory, 'collated_short_interest_data.csv')\n",
    "\n",
    "# Check if the collated file already exists and delete it\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Get a list of all pipe-delimited CSV files in the directory\n",
    "csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Read and concatenate all CSV files with proper delimiter handling\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep='|')  # Read as pipe-delimited with specific dtype\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "if df_list:\n",
    "    short_interest_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Replace daysToCoverQuantity with blank where averageDailyVolumeQuantity is 0\n",
    "    short_interest_df.loc[short_interest_df['averageDailyVolumeQuantity'] == 0, 'daysToCoverQuantity'] = None\n",
    "    \n",
    "    # Remove all entries where the ticker (symbolCode) is missing and daysToCoverQuantity is NaN or 999.99 \n",
    "    short_interest_df = short_interest_df.dropna(subset=['symbolCode', 'daysToCoverQuantity'])\n",
    "    short_interest_df = short_interest_df[short_interest_df['daysToCoverQuantity'] != 999.99]\n",
    "\n",
    "    # Keep only stocks not traded Over the Counter\n",
    "    short_interest_df = short_interest_df[short_interest_df['marketClassCode'] != 'OTC']\n",
    "\n",
    "    # Drop unnecessary fields\n",
    "    short_interest_df = short_interest_df.drop(columns=['accountingYearMonthNumber', 'issuerServicesGroupExchangeCode', 'stockSplitFlag', 'revisionFlag', 'changePercent', 'changePreviousNumber', 'previousShortPositionQuantity', 'issueName', 'marketClassCode'])\n",
    "    \n",
    "    # Move settlementDate to the first column\n",
    "    columns = ['settlementDate'] + [col for col in short_interest_df.columns if col != 'settlementDate']\n",
    "    short_interest_df = short_interest_df[columns]\n",
    "\n",
    "    # Sort by settlementDate and symbolCode\n",
    "    short_interest_df = short_interest_df.sort_values(by=['settlementDate', 'symbolCode'])\n",
    "\n",
    "    # Renaming columns\n",
    "    short_interest_df.rename(columns={\"currentShortPositionQuantity\": \"short_volume\"}, inplace=True)\n",
    "    short_interest_df.rename(columns={\"averageDailyVolumeQuantity\": \"avg_daily_volume\"}, inplace=True)\n",
    "    short_interest_df.rename(columns={\"daysToCoverQuantity\": \"days_to_cover\"}, inplace=True)\n",
    "\n",
    "    # Chaging fields to appropriate data type\n",
    "    short_interest_df = short_interest_df.astype({\"short_volume\": \"int32\", \"avg_daily_volume\": \"int32\"})\n",
    "\n",
    "    # Save the collated DataFrame to the same directory\n",
    "    short_interest_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Collated data saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No valid CSV files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Open .csv file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = 'data/finra_short_interest_data'\n",
    "\n",
    "# Open the collated file in a DataFrame for viewing\n",
    "short_interest_file = os.path.join(directory, 'collated_short_interest_data.csv')\n",
    "short_interest_df = pd.read_csv(short_interest_file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tickers and date ranges to use as parameters for remaining data extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16857 unique tickers in the short interest file\n",
      "Short interest file date range is 2021-06-15 to 2025-01-15\n"
     ]
    }
   ],
   "source": [
    "# Extract unique tickers from the short interest file. Will be used as the main variable to pass through to subsquent queries where tickers are required\n",
    "tickers = short_interest_df['symbolCode'].unique().tolist()\n",
    "print(f\"{len(tickers)} unique tickers in the short interest file\")\n",
    "\n",
    "# Extract earliest and latest date in the short interest file\n",
    "earliest_date = '2020-01-01'\n",
    "latest_date = short_interest_df['settlementDate'].max()\n",
    "\n",
    "short_interest_start_date = short_interest_df['settlementDate'].min()\n",
    "\n",
    "print(f\"Short interest file date range is {short_interest_start_date} to {latest_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. WRDS (Wharton) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Company Data (Quarterly) - Fundamentals\n",
    "https://wrds-www.wharton.upenn.edu/pages/get-data/compustat-capital-iq-standard-poors/compustat/north-america-daily/fundamentals-quarterly/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract quarterly financial statement data and derive commonly used metrics if not available directly.\n",
    "\n",
    "Variable references (for the quarterly reporting period, in USD):\n",
    "- conm: company name\n",
    "- tic: company ticker symbol\n",
    "- rdq: report date of quarterly earnings\n",
    "- revtq: total revenue \n",
    "- cogsq: cost of goods sold\n",
    "- oiadpq: operating income after depreciation and amortisation\n",
    "- dlcq: short-term (current) debt\n",
    "- dlttq: long-term debt\n",
    "- cheq: cash and cash equivalents at reporting point in time\n",
    "\n",
    "The variables above will be used to calculated the following metrics:\n",
    "\n",
    "- Gross Margin = (revtq – cogsq) / revtq \n",
    "    - \"revtq\" represents total revenues and \"cogsq\" represents the Cost of Goods Sold both at quarter level. The difference equals gross profit.\n",
    "- EBITDA = oiadpq + dpq\n",
    "    - Earnings Before Interest, Tax, Depreciation and Amortization. Since oiadpq already deducts depreciation and amortisation, adding dpq back returns EBITDA.\n",
    "- Net Debt = (dlcq + dlttq) – che\n",
    "    - Net Debt measures a company’s overall debt situation by offsetting its total debt with its liquid assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download data and save it as a .csv so we avoiding repeated long queries to WRDS in case we clear memory. \n",
    "If the data has already been downloaded do not run this and skip to b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to download into\n",
    "directory = 'data/wrds_company_fundamentals_data'\n",
    "output_file = os.path.join(directory, 'wrds_company_fundamentals_data.csv')\n",
    "\n",
    "# Check if the collated file already exists and delete it\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Pass parameters to a sql statement\n",
    "params = {\n",
    "    \"tickers\": tuple(tickers),\n",
    "    \"start_date\": earliest_date,\n",
    "    \"end_date\": latest_date\n",
    "}\n",
    "\n",
    "# Query WRDS to fetch data\n",
    "quartely_company_fundamentals_df = db.raw_sql(\n",
    "    \"SELECT rdq as date, tic as ticker, revtq as revenue, ceqq as book_value, niq as net_income, oiadpq as op_income, dlcq as st_debt, dlttq as lt_debt, cheq as cash_eq, atq as total_assets, \"\n",
    "    \"(revtq - cogsq) / NULLIF(revtq, 0) as gross_margin, (revtq - cogsq) / NULLIF(atq, 0) as gross_profitability, \"\n",
    "    \"oiadpq / NULLIF(atq, 0) as operating_profitability, (dlcq + dlttq) / NULLIF(atq, 0) as leverage, dlcq + dlttq - cheq as net_debt, oiadpq + dpq as ebitda, \"\n",
    "    \"(dlcq + dlttq - cheq) / NULLIF((oiadpq + dpq), 0) as netdebt_to_ebitda \" \n",
    "    \"FROM comp_na_daily_current.fundq \" \n",
    "    \"WHERE tic in %(tickers)s and rdq BETWEEN %(start_date)s AND %(end_date)s \",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# Save the DataFrame to the directory\n",
    "quartely_company_fundamentals_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Open .csv file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = 'data/wrds_company_fundamentals_data'\n",
    "\n",
    "# Open the file in a DataFrame for viewing\n",
    "quartely_company_fundamentals_file = os.path.join(directory, 'wrds_company_fundamentals_data.csv')\n",
    "quartely_company_fundamentals_df = pd.read_csv(quartely_company_fundamentals_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract unique downloaded tickers and compare to unique ticker list derived so far. Delete unmatched tickers from previous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6138 unique tickers found in WRDS quarterly company fundamentals data\n",
      "16857 unique tickers found in FINRA short interest data\n",
      "10719 tickers missing from the WRDS quarterly company fundamentals compared to the short interest file\n",
      "Updated ticker list. New unique ticker count is: 6138\n"
     ]
    }
   ],
   "source": [
    "# Extract unique tickers from WRDS download\n",
    "quartely_company_fundamentals_tickers = set(quartely_company_fundamentals_df['ticker'].unique().tolist())\n",
    "print(f\"{len(quartely_company_fundamentals_tickers)} unique tickers found in WRDS quarterly company fundamentals data\")\n",
    "\n",
    "# Find missing tickers (tickers in short_interest_df but NOT in quartely_company_fundamentals_df)\n",
    "print(f\"{len(tickers)} unique tickers found in FINRA short interest data\")\n",
    "missing_tickers = set(tickers) - quartely_company_fundamentals_tickers\n",
    "\n",
    "print(f\"{len(missing_tickers)} tickers missing from the WRDS quarterly company fundamentals compared to the short interest file\")\n",
    "\n",
    "# Remove records with missing tickers from short_interest_df and daily stock data\n",
    "short_interest_df = short_interest_df[~short_interest_df['symbolCode'].isin(missing_tickers)]\n",
    "\n",
    "# Update ticker variable\n",
    "tickers = short_interest_df['symbolCode'].unique().tolist()\n",
    "print(f\"Updated ticker list. New unique ticker count is: {len(tickers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Stock Data (Daily Level) - Prices and Volume\n",
    "https://wrds-www.wharton.upenn.edu/pages/get-data/compustat-capital-iq-standard-poors/compustat/north-america-daily/security-daily/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Reference:\n",
    "- conm: company name\n",
    "- datadate: record date\n",
    "- tic: ticker symbol\n",
    "- cshoc: shares outstanding\n",
    "- cshtrd: trading Volume - daily\n",
    "- prccd: price - close - daily\n",
    "\n",
    "Filtering the data to fetch only USA stocks in order to be able to inspect the dataset sensibly without the need for currency conversions and consistency of financial statement data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download data and save it as a .csv so we avoiding repeated long queries to WRDS in case we clear memory. \n",
    "If the data has already been downloaded do not run this and skip to b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to download into\n",
    "directory = 'data/wrds_stock_daily_data'\n",
    "output_file = os.path.join(directory, 'wrds_stock_daily_data.csv')\n",
    "\n",
    "# Check if the collated file already exists and delete it\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Pass parameters to a sql statement\n",
    "params = {\n",
    "    \"tickers\": tuple(tickers),\n",
    "    \"start_date\": earliest_date,\n",
    "    \"end_date\": latest_date\n",
    "}\n",
    "\n",
    "# Query WRDS to fetch data\n",
    "daily_stock_data_df = db.raw_sql(\n",
    "    \"SELECT datadate, conm as company_name, tic as ticker, prccd as price_close, cshtrd as volume, cshoc as shares_outstanding, prccd * cshoc as market_cap, eps \" \n",
    "    \"FROM comp_na_daily_all.secd \" \n",
    "    \"WHERE tic in %(tickers)s and datadate BETWEEN %(start_date)s AND %(end_date)s AND fic = 'USA'\",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# Save the DataFrame to the directory\n",
    "daily_stock_data_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Open .csv file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = 'data/wrds_stock_daily_data'\n",
    "\n",
    "# Open the file in a DataFrame for viewing\n",
    "daily_stock_data_file = os.path.join(directory, 'wrds_stock_daily_data.csv')\n",
    "daily_stock_data_df = pd.read_csv(daily_stock_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract unique downloaded tickers and compare to short interest file dataset. Delete unmatched tickers from previous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4740 unique tickers found in WRDS daily stock data\n",
      "6138 unique tickers found in FINRA short interest data\n",
      "1398 tickers missing from the WRDS daily stock data compared to the short interest file\n",
      "Updated ticker list. New unique ticker count is: 4740\n"
     ]
    }
   ],
   "source": [
    "# Extract unique tickers from WRDS download\n",
    "daily_stock_data_tickers = set(daily_stock_data_df['ticker'].unique().tolist())\n",
    "print(f\"{len(daily_stock_data_tickers)} unique tickers found in WRDS daily stock data\")\n",
    "\n",
    "# Find missing tickers (tickers in ticker list but NOT in daily_stock_data_df)\n",
    "print(f\"{len(tickers)} unique tickers found in FINRA short interest data\")\n",
    "missing_tickers = set(tickers) - daily_stock_data_tickers\n",
    "\n",
    "print(f\"{len(missing_tickers)} tickers missing from the WRDS daily stock data compared to the short interest file\")\n",
    "\n",
    "# Remove records with missing tickers from short_interest_df and quartely_company_fundamentals_df\n",
    "short_interest_df = short_interest_df[~short_interest_df['symbolCode'].isin(missing_tickers)]\n",
    "quartely_company_fundamentals_df = quartely_company_fundamentals_df[~quartely_company_fundamentals_df['ticker'].isin(missing_tickers)]\n",
    "\n",
    "# Update ticker variable\n",
    "tickers = short_interest_df['symbolCode'].unique().tolist()\n",
    "print(f\"Updated ticker list. New unique ticker count is: {len(tickers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Yahoo Finance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Market Data - S&P500 and VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = 'data/yahoo_finance_sp500_vix_data'\n",
    "output_file = os.path.join(directory, 'yahoo_finance_sp500_vix_data.csv')\n",
    "\n",
    "# Check if the collated file already exists and delete it\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Define ticker symbols\n",
    "tickers = [\"^GSPC\", \"^VIX\"]  # S&P 500 and VIX\n",
    "\n",
    "# Fetch data (Closing Prices and Volume)\n",
    "sp500_vix_data_df = yf.download(tickers, start=earliest_date, end=latest_date, progress=False)[['Close']]\n",
    "\n",
    "# Flatten the MultiIndex to standard column names\n",
    "sp500_vix_data_df.columns = [f\"{col[0]}_{col[1]}\" for col in sp500_vix_data_df.columns]\n",
    "\n",
    "# Rename columns to match the requested format\n",
    "sp500_vix_data_df = sp500_vix_data_df.rename(columns={\n",
    "    'Close_^GSPC': 'sp500_price_close',\n",
    "    'Close_^VIX': 'vix_price_close'\n",
    "})\n",
    "\n",
    "# Reorder columns\n",
    "sp500_vix_data_df = sp500_vix_data_df[['sp500_price_close', 'vix_price_close']]\n",
    "\n",
    "# Format date index to YYYY-MM-DD and reset index\n",
    "sp500_vix_data_df.index = sp500_vix_data_df.index.strftime('%Y-%m-%d')\n",
    "sp500_vix_data_df = sp500_vix_data_df.reset_index()\n",
    "\n",
    "# Rename the date column to 'date'\n",
    "sp500_vix_data_df = sp500_vix_data_df.rename(columns={'index': 'date'})\n",
    "\n",
    "# Save the collated DataFrame to the same directory\n",
    "sp500_vix_data_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Open .csv file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = 'data/yahoo_finance_sp500_vix_data'\n",
    "\n",
    "# Open the collated file in a DataFrame for viewing\n",
    "sp500_vix_data_file = os.path.join(directory, 'yahoo_finance_sp500_vix_data.csv')\n",
    "sp500_vix_data_df = pd.read_csv(sp500_vix_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Join datasets, perform data cleansing and compute additional metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Join datasets and foward fill datapoints where relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of tickers with duplicate (ticker, date) pairs: 507\n",
      "4233 unique tickers after joining datasets\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns used as join criteria between datasets\n",
    "daily_stock_data_df.rename(columns={\"datadate\": \"date\"}, inplace=True)\n",
    "sp500_vix_data_df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "short_interest_df.rename(columns={\"settlementDate\": \"date\"}, inplace=True)\n",
    "short_interest_df.rename(columns={\"symbolCode\": \"ticker\"}, inplace=True)\n",
    "\n",
    "# Merge stock data and index data on date\n",
    "merged_df = pd.merge(daily_stock_data_df, sp500_vix_data_df, on=\"date\", how=\"left\")\n",
    "\n",
    "# Merge ticker-level data (on date + ticker)\n",
    "merged_df = pd.merge(merged_df, quartely_company_fundamentals_df, on=[\"date\", \"ticker\"], how=\"left\")\n",
    "merged_df = pd.merge(merged_df, short_interest_df, on=[\"date\", \"ticker\"], how=\"left\")\n",
    "\n",
    "# Sort by date and ticker\n",
    "merged_df = merged_df.sort_values(by=[\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "# Group by 'ticker' and 'date' and count the number of occurrences.\n",
    "dup_counts = merged_df.groupby(['ticker', 'date']).size().reset_index(name='count')\n",
    "\n",
    "# Identify (ticker, date) pairs with more than 1 occurrence.\n",
    "dup_pairs = dup_counts[dup_counts['count'] > 1]\n",
    "\n",
    "# Extract the unique tickers that have duplicate (ticker, date) pairs.\n",
    "tickers_with_duplicates = dup_pairs['ticker'].unique()\n",
    "print(\"Count of tickers with duplicate (ticker, date) pairs:\", len(tickers_with_duplicates))\n",
    "\n",
    "# Filter out any rows where the 'ticker' is in the tickers_with_duplicates list.\n",
    "merged_df = merged_df[~merged_df['ticker'].isin(tickers_with_duplicates)].copy()\n",
    "\n",
    "# Forward fill ticker-specific, point-in-time values (e.g. short interest, quarterly gross profit, etc.)\n",
    "ticker_cols = list(merged_df.columns)\n",
    "ticker_cols.remove(\"date\")  # Exclude date column from filling\n",
    "ticker_cols.remove(\"ticker\")  # Exclude ticker column from filling\n",
    "merged_df[ticker_cols] = merged_df.groupby(\"ticker\")[ticker_cols].ffill()\n",
    "\n",
    "# Scale relevant fields from short interest file and daily stock data to millions in order to match the fundamentals file\n",
    "divisor = 1_000_000\n",
    "cols_to_divide = ['volume', 'shares_outstanding', 'market_cap', 'short_volume', 'avg_daily_volume']\n",
    "\n",
    "merged_df[cols_to_divide] = merged_df[cols_to_divide] / divisor\n",
    "\n",
    "# Ticker count after joining\n",
    "tickers_post_joins = merged_df['ticker'].unique().tolist()\n",
    "print(f\"{len(tickers_post_joins)} unique tickers after joining datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Adjust for possible stock splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the DataFrame is sorted by ticker and date.\n",
    "merged_df = merged_df.sort_values(['ticker', 'date'])\n",
    "\n",
    "# Step 1: Compute the previous day's shares outstanding within each ticker.\n",
    "merged_df['prev_shares'] = merged_df.groupby('ticker')['shares_outstanding'].shift(1)\n",
    "# For the first observation of each ticker, fill missing value with the current day's shares.\n",
    "merged_df['prev_shares'] = merged_df['prev_shares'].fillna(merged_df['shares_outstanding'])\n",
    "\n",
    "# Step 2: Compute the day-to-day ratio of shares outstanding.\n",
    "merged_df['shares_ratio'] = merged_df['shares_outstanding'] / merged_df['prev_shares']\n",
    "\n",
    "# Step 3: Define a threshold to detect a split.\n",
    "# If the shares_ratio is greater than or equal to the threshold (e.g., 1.5), assume a split occurred.\n",
    "split_threshold = 1.5\n",
    "\n",
    "# For days when a split is detected, compute a multiplier as 1 / shares_ratio.\n",
    "# Otherwise, the multiplier is 1.\n",
    "merged_df['split_multiplier'] = np.where(\n",
    "    (merged_df['shares_ratio'] >= split_threshold).fillna(False),\n",
    "    1 / merged_df['shares_ratio'],\n",
    "    1.0\n",
    ")\n",
    "\n",
    "# Step 4: Shift the split multiplier backward (i.e. forward in time)\n",
    "# so that the multiplier detected on the split day is applied to the previous day.\n",
    "# This means that the day on which the split is reported is treated as the new baseline.\n",
    "merged_df['split_multiplier_shifted'] = merged_df.groupby('ticker')['split_multiplier'].shift(-1).fillna(1.0)\n",
    "\n",
    "# Step 5: Compute the reverse cumulative product of the shifted multiplier within each ticker.\n",
    "def reverse_cumprod(series):\n",
    "    return series.iloc[::-1].cumprod().iloc[::-1]\n",
    "\n",
    "merged_df['adjustment_factor'] = merged_df.groupby('ticker')['split_multiplier_shifted'].transform(reverse_cumprod)\n",
    "\n",
    "# Step 6: Compute the split-adjusted price.\n",
    "merged_df['price_close_adj'] = merged_df['price_close'] * merged_df['adjustment_factor']\n",
    "\n",
    "# Optional: Clean up the temporary columns.\n",
    "# merged_df.drop(columns=['prev_shares', 'shares_ratio', 'split_multiplier', 'split_multiplier_shifted', 'adjustment_factor'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Fundamental Ratios\n",
    "# ==========================================\n",
    "\n",
    "# Price-to-Earnings (P/E) Ratio: Only compute if EPS != 0; otherwise, set to NaN.\n",
    "merged_df['pe_ratio'] = np.where((merged_df['eps'] != 0).fillna(False),\n",
    "                                 merged_df['price_close'] / merged_df['eps'], # can use non-adjusted price_close as eps is as reported at that point in time\n",
    "                                 np.nan)\n",
    "\n",
    "# Price-to-Book (P/B) Ratio: Only compute if book_value != 0; otherwise, set to NaN.\n",
    "merged_df['pb_ratio'] = np.where((merged_df['book_value'] != 0).fillna(False),\n",
    "                                 merged_df['market_cap'] / merged_df['book_value'],\n",
    "                                 np.nan)\n",
    "\n",
    "# Price-to-Sales (P/S) Ratio: Only compute if revenue != 0; otherwise, set to NaN.\n",
    "merged_df['ps_ratio'] = np.where((merged_df['revenue'] != 0).fillna(False),\n",
    "                                 merged_df['market_cap'] / merged_df['revenue'],\n",
    "                                 np.nan)\n",
    "\n",
    "# Price-to-EBITDA (P/EBITDA) Ratio: Only compute if EBITDA != 0; otherwise, set to NaN.\n",
    "merged_df['pebitda_ratio'] = np.where((merged_df['ebitda'] != 0).fillna(False),\n",
    "                                      merged_df['market_cap'] / merged_df['ebitda'],\n",
    "                                      np.nan)\n",
    "\n",
    "\n",
    "# Enterprise Value (EV) and EV/EBITDA:\n",
    "\n",
    "# Compute EV as market_cap + net_debt.\n",
    "merged_df['enterprise_value'] = merged_df['market_cap'] + merged_df['net_debt']\n",
    "\n",
    "# Only compute EV/EBITDA if EBITDA != 0.\n",
    "merged_df['ev_ebitda'] = np.where((merged_df['ebitda'] != 0).fillna(False),\n",
    "                                  merged_df['enterprise_value'] / merged_df['ebitda'],\n",
    "                                  np.nan)\n",
    "\n",
    "\n",
    "# Return on Equity (ROE): net_income / book_value\n",
    "merged_df['roe'] = merged_df['net_income'] / merged_df['book_value'].replace({0: np.nan})\n",
    "\n",
    "# Net Margin: net_income / revenue\n",
    "merged_df['net_margin'] = merged_df['net_income'] / merged_df['revenue'].replace({0: np.nan})\n",
    "\n",
    "# ==========================================\n",
    "# 2. Growth Metrics\n",
    "# ==========================================\n",
    "\n",
    "def compute_quarterly_growth(series):\n",
    "    \"\"\"\n",
    "    Computes quarterly growth for a forward-filled series.\n",
    "    The function calculates the percentage change only on days when the value changes\n",
    "    (i.e., a new quarterly report is available), and then forward fills that value\n",
    "    until the next change.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask: True when the reported value is different from the previous day.\n",
    "    mask = series != series.shift(1)\n",
    "    \n",
    "    # Compute the percentage change (growth) on a day-by-day basis.\n",
    "    growth = series.pct_change(fill_method=None)\n",
    "    \n",
    "    # Only keep the computed growth on the change days; elsewhere, set to NaN.\n",
    "    growth = growth.where(mask)\n",
    "    \n",
    "    # Forward fill the computed growth so that every day between reports has the same growth value.\n",
    "    growth = growth.ffill()\n",
    "    \n",
    "    return growth\n",
    "\n",
    "# Compute quarterly growth for EPS and revenue by grouping on ticker.\n",
    "merged_df['eps_growth'] = merged_df.groupby('ticker')['eps'].transform(compute_quarterly_growth)\n",
    "merged_df['revenue_growth'] = merged_df.groupby('ticker')['revenue'].transform(compute_quarterly_growth)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Volatility Metrics\n",
    "# ==========================================\n",
    "\n",
    "# Compute daily returns from price_close with fill_method=None to avoid the warning.\n",
    "merged_df['daily_return'] = merged_df['price_close_adj'].pct_change(fill_method=None)\n",
    "\n",
    "# 30-day rolling volatility (standard deviation of daily returns)\n",
    "merged_df['volatility_30d'] = merged_df['daily_return'].rolling(window=30, min_periods=15).std()\n",
    "\n",
    "# 90-day rolling volatility\n",
    "merged_df['volatility_90d'] = merged_df['daily_return'].rolling(window=90, min_periods=45).std()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Momentum Metrics\n",
    "# ==========================================\n",
    "\n",
    "# 10 day momentum: cumulative return over the past 10 trading days.\n",
    "merged_df['momentum_10d'] = merged_df.groupby('ticker')['price_close_adj'].pct_change(periods=10, fill_method=None)\n",
    "\n",
    "# 20 day momentum: cumulative return over the past 20 trading days.\n",
    "merged_df['momentum_20d'] = merged_df.groupby('ticker')['price_close_adj'].pct_change(periods=20, fill_method=None)\n",
    "\n",
    "# 50 day momentum: cumulative return over the past 50 trading days.\n",
    "merged_df['momentum_50d'] = merged_df.groupby('ticker')['price_close_adj'].pct_change(periods=50, fill_method=None)\n",
    "\n",
    "# 100 day momentum: cumulative return over the past 100 trading days.\n",
    "merged_df['momentum_100d'] = merged_df.groupby('ticker')['price_close_adj'].pct_change(periods=100, fill_method=None)\n",
    "\n",
    "# 200 day momentum: cumulative return over the past 200 trading days.\n",
    "merged_df['momentum_200d'] = merged_df.groupby('ticker')['price_close_adj'].pct_change(periods=200, fill_method=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Remove rows before cutoff date (short interest file start date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final count of tickers is: 4233\n"
     ]
    }
   ],
   "source": [
    "# Delete rows\n",
    "merged_df = merged_df[merged_df['date'] >= short_interest_start_date].copy()\n",
    "tickers_final = merged_df['ticker'].unique().tolist()\n",
    "print(f\"Final count of tickers is: {len(tickers_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Cleanse dataset and improve readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of tickers with 0 or missing shares outstanding: 26\n",
      "DataFrame shape after removing tickers with 0 or missing shares outstanding: (3440258, 52)\n",
      "Count of tickers where market cap has been below $100M: 485\n",
      "DataFrame shape after removing tickers with low market cap: (3031766, 52)\n",
      "Count of tickers missing revenue, book_value or eps: 778\n",
      "DataFrame shape after removing tickers with missing revenue, book_value or eps: (2431749, 52)\n",
      "Count of tickers missing lt_debt or st_debt: 80\n",
      "DataFrame shape after removing tickers with missing lt_debt or st_debt: (2368027, 52)\n",
      "Count of tickers post cleanup is: 2864\n"
     ]
    }
   ],
   "source": [
    "# Identify tickers with shares_outstanding == 0 or NaN.\n",
    "tickers_shares_zero_or_missing = merged_df.loc[(merged_df['shares_outstanding'] == 0) | (merged_df['shares_outstanding'].isna()),'ticker'].unique()\n",
    "print(\"Count of tickers with 0 or missing shares outstanding:\", len(tickers_shares_zero_or_missing))\n",
    "\n",
    "merged_df = merged_df[~merged_df['ticker'].isin(tickers_shares_zero_or_missing)].copy()\n",
    "print(\"DataFrame shape after removing tickers with 0 or missing shares outstanding:\", merged_df.shape)\n",
    "\n",
    "# Identify and remove tickers where market capitalisation has been below $10M.\n",
    "tickers_low_market_cap = merged_df.loc[merged_df['market_cap'] < 10, 'ticker'].unique()\n",
    "print(\"Count of tickers where market cap has been below $100M:\", len(tickers_low_market_cap))\n",
    "\n",
    "merged_df = merged_df[~merged_df['ticker'].isin(tickers_low_market_cap)].copy()\n",
    "print(\"DataFrame shape after removing tickers with low market cap:\", merged_df.shape)\n",
    "\n",
    "# Identify tickers with either revenue / book_value / eps == NaN.\n",
    "tickers_missing_rev_bk_eps = merged_df.loc[(merged_df['revenue'].isna()) | (merged_df['book_value'].isna()) | (merged_df['eps'].isna()),'ticker'].unique()\n",
    "print(\"Count of tickers missing revenue, book_value or eps:\", len(tickers_missing_rev_bk_eps))\n",
    "\n",
    "merged_df = merged_df[~merged_df['ticker'].isin(tickers_missing_rev_bk_eps)].copy()\n",
    "print(\"DataFrame shape after removing tickers with missing revenue, book_value or eps:\", merged_df.shape)\n",
    "\n",
    "# Identify tickers with either lt_debt / st_debt / total_assets == NaN.\n",
    "tickers_missing_debt_assets = merged_df.loc[(merged_df['lt_debt'].isna()) | (merged_df['st_debt'].isna()) | (merged_df['total_assets'].isna()),'ticker'].unique()\n",
    "print(\"Count of tickers missing lt_debt, st_debt or total assets:\", len(tickers_missing_debt_assets))\n",
    "\n",
    "merged_df = merged_df[~merged_df['ticker'].isin(tickers_missing_debt_assets)].copy()\n",
    "print(\"DataFrame shape after removing tickers with missing lt_debt, st_debt or total_assets:\", merged_df.shape)\n",
    "\n",
    "# Count of tickers post cleanup\n",
    "tickers_cleanup = merged_df['ticker'].unique().tolist()\n",
    "print(f\"Count of tickers post cleanup is: {len(tickers_cleanup)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>price_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>eps</th>\n",
       "      <th>sp500_price_close</th>\n",
       "      <th>vix_price_close</th>\n",
       "      <th>revenue</th>\n",
       "      <th>book_value</th>\n",
       "      <th>net_income</th>\n",
       "      <th>op_income</th>\n",
       "      <th>st_debt</th>\n",
       "      <th>lt_debt</th>\n",
       "      <th>cash_eq</th>\n",
       "      <th>total_assets</th>\n",
       "      <th>gross_margin</th>\n",
       "      <th>gross_profitability</th>\n",
       "      <th>operating_profitability</th>\n",
       "      <th>leverage</th>\n",
       "      <th>net_debt</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>netdebt_to_ebitda</th>\n",
       "      <th>short_volume</th>\n",
       "      <th>avg_daily_volume</th>\n",
       "      <th>days_to_cover</th>\n",
       "      <th>prev_shares</th>\n",
       "      <th>shares_ratio</th>\n",
       "      <th>split_multiplier</th>\n",
       "      <th>split_multiplier_shifted</th>\n",
       "      <th>adjustment_factor</th>\n",
       "      <th>price_close_adj</th>\n",
       "      <th>pe_ratio</th>\n",
       "      <th>pb_ratio</th>\n",
       "      <th>ps_ratio</th>\n",
       "      <th>pebitda_ratio</th>\n",
       "      <th>enterprise_value</th>\n",
       "      <th>ev_ebitda</th>\n",
       "      <th>roe</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>eps_growth</th>\n",
       "      <th>revenue_growth</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>volatility_30d</th>\n",
       "      <th>volatility_90d</th>\n",
       "      <th>momentum_10d</th>\n",
       "      <th>momentum_20d</th>\n",
       "      <th>momentum_50d</th>\n",
       "      <th>momentum_100d</th>\n",
       "      <th>momentum_200d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5174153</th>\n",
       "      <td>2025-01-15</td>\n",
       "      <td>NVIDIA CORP</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>136.24</td>\n",
       "      <td>183.75</td>\n",
       "      <td>24,490.00</td>\n",
       "      <td>3,336,517.60</td>\n",
       "      <td>2.56</td>\n",
       "      <td>5,842.91</td>\n",
       "      <td>18.71</td>\n",
       "      <td>35,082.00</td>\n",
       "      <td>65,899.00</td>\n",
       "      <td>19,309.00</td>\n",
       "      <td>21,868.00</td>\n",
       "      <td>273.00</td>\n",
       "      <td>9,952.00</td>\n",
       "      <td>38,487.00</td>\n",
       "      <td>96,013.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-28,262.00</td>\n",
       "      <td>22,346.00</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>287.06</td>\n",
       "      <td>228.36</td>\n",
       "      <td>1.26</td>\n",
       "      <td>24,490.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>136.24</td>\n",
       "      <td>53.12</td>\n",
       "      <td>50.63</td>\n",
       "      <td>95.11</td>\n",
       "      <td>149.31</td>\n",
       "      <td>3,308,255.60</td>\n",
       "      <td>148.05</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date company_name ticker  price_close  volume  \\\n",
       "5174153  2025-01-15  NVIDIA CORP   NVDA       136.24  183.75   \n",
       "\n",
       "         shares_outstanding   market_cap  eps  sp500_price_close  \\\n",
       "5174153           24,490.00 3,336,517.60 2.56           5,842.91   \n",
       "\n",
       "         vix_price_close   revenue  book_value  net_income  op_income  \\\n",
       "5174153            18.71 35,082.00   65,899.00   19,309.00  21,868.00   \n",
       "\n",
       "         st_debt  lt_debt   cash_eq  total_assets  gross_margin  \\\n",
       "5174153   273.00 9,952.00 38,487.00     96,013.00          0.76   \n",
       "\n",
       "         gross_profitability  operating_profitability  leverage   net_debt  \\\n",
       "5174153                 0.28                     0.23      0.11 -28,262.00   \n",
       "\n",
       "           ebitda  netdebt_to_ebitda  short_volume  avg_daily_volume  \\\n",
       "5174153 22,346.00              -1.26        287.06            228.36   \n",
       "\n",
       "         days_to_cover  prev_shares  shares_ratio  split_multiplier  \\\n",
       "5174153           1.26    24,490.00          1.00              1.00   \n",
       "\n",
       "         split_multiplier_shifted  adjustment_factor  price_close_adj  \\\n",
       "5174153                      1.00               1.00           136.24   \n",
       "\n",
       "         pe_ratio  pb_ratio  ps_ratio  pebitda_ratio  enterprise_value  \\\n",
       "5174153     53.12     50.63     95.11         149.31      3,308,255.60   \n",
       "\n",
       "         ev_ebitda  roe  net_margin  eps_growth  revenue_growth  daily_return  \\\n",
       "5174153     148.05 0.29        0.55        0.19            0.17          0.03   \n",
       "\n",
       "         volatility_30d  volatility_90d  momentum_10d  momentum_20d  \\\n",
       "5174153            0.00            0.17         -0.01          0.01   \n",
       "\n",
       "         momentum_50d  momentum_100d  momentum_200d  \n",
       "5174153          0.03           0.06           0.51  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Display a sample of the resulting DataFrame with new factors and check split adjustment\n",
    "# ==========================================\n",
    "\n",
    "selected_columns = ['date', 'ticker', 'price_close', 'price_close_adj', 'eps', 'pe_ratio', 'pb_ratio', 'ps_ratio', 'pebitda_ratio', \n",
    "                    'ev_ebitda', 'roe', 'net_margin', 'eps_growth', 'revenue_growth', \n",
    "                    'volatility_30d', 'volatility_90d', 'momentum_10d', 'momentum_50d', 'momentum_200d', 'shares_outstanding', 'prev_shares', 'shares_ratio', 'split_multiplier', 'split_multiplier_shifted', 'adjustment_factor']\n",
    "\n",
    "merged_df.loc[merged_df['ticker'] == 'NVDA'].sort_values(by=[\"date\"]).tail(1) #use SMCI or NVIDIA as they've had recent splits\n",
    "\n",
    "# # Switch column order for better readability\n",
    "# new_order = ['date', 'ticker', 'company_name', 'price_close', 'shares_outstanding', 'market_cap', 'volume', 'short_volume', 'avg_daily_volume', 'days_to_cover',\n",
    "#              'sp500_price_close', 'vix_price_close', 'eps', 'book_value', 'revenue', 'gross_margin', 'gross_profitability', 'ebitda', 'operating_profitability', \n",
    "#              'net_income', 'net_debt', 'netdebt_to_ebitda', 'leverage'\n",
    "#             ]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "# merged_df = merged_df[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                             0\n",
       "company_name                     0\n",
       "ticker                           0\n",
       "price_close                      0\n",
       "volume                           0\n",
       "shares_outstanding               0\n",
       "market_cap                       0\n",
       "eps                              0\n",
       "sp500_price_close                0\n",
       "vix_price_close                  0\n",
       "revenue                          0\n",
       "book_value                       0\n",
       "net_income                       0\n",
       "op_income                        0\n",
       "st_debt                          0\n",
       "lt_debt                          0\n",
       "cash_eq                          0\n",
       "total_assets                     0\n",
       "gross_margin                 68923\n",
       "gross_profitability              0\n",
       "operating_profitability          0\n",
       "leverage                         0\n",
       "net_debt                         0\n",
       "ebitda                      131400\n",
       "netdebt_to_ebitda           132004\n",
       "short_volume                 59996\n",
       "avg_daily_volume             59996\n",
       "days_to_cover                59996\n",
       "prev_shares                      0\n",
       "shares_ratio                     0\n",
       "split_multiplier                 0\n",
       "split_multiplier_shifted         0\n",
       "adjustment_factor                0\n",
       "price_close_adj                  0\n",
       "pe_ratio                      4999\n",
       "pb_ratio                         0\n",
       "ps_ratio                    107260\n",
       "pebitda_ratio               131431\n",
       "enterprise_value                 0\n",
       "ev_ebitda                   131431\n",
       "roe                              0\n",
       "net_margin                  107260\n",
       "eps_growth                    4415\n",
       "revenue_growth               69444\n",
       "daily_return                     0\n",
       "volatility_30d                   0\n",
       "volatility_90d                   0\n",
       "momentum_10d                     0\n",
       "momentum_20d                     0\n",
       "momentum_50d                     0\n",
       "momentum_100d                  292\n",
       "momentum_200d                 4212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
